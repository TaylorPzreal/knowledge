

Review
1. 2023-07-09 19:05

## 一、Introduction
生成式预训练 Transformer（GPT）是 OpenAI 开发在自然语言处理（NLP）领域的创新之举。OpenAI是MaaS（Model-as-a-Service）的先行者。

1. GPT-2 是 OpenAI 在 2019 年 2 月创建的一种基于 Transformer 的无监督深度学习语言模型，其目的只有一个，就是预测句子中的下一个单词。GPT-2 是“Generative Pretrained Transformer 2”的缩写。该模型是开源的，在超过 15 亿个参数上进行训练，以便为给定句子生成下一个文本序列。
2. GPT-3 就是“生成式预训练 Transformer”，它是 GPT-2 的第 3 个发行版，也是一个升级版。第 3 版将 GPT 模型提升到了一个全新的高度，因为它的训练参数达到了 1750 亿个（这是前代 GPT-2 的 10 倍以上）。GPT-3 是在一个名为“Common Crawl”的开源数据集上进行训练的，还有来自 OpenAI 的其他文本，如维基百科（Wikipedia）条目。

GPT-3 的创建是为了比 GPT-2 更强大，因为它能够处理更多的特定主题。GPT-2 在接受音乐和讲故事等专业领域的任务时表现不佳，这是众所周知的。现在，GPT-3 可以更进一步地完成诸如答题、写论文、文本摘要、语言翻译和生成计算机代码等任务。它能够生成计算机代码，本身就已经是一个重大的壮举了。

GPT-3 通过语义学的方法理解语言的含义，并尝试输出一个有意义的句子给用户，从而在接受输入后生成句子。因为不使用标签化的数据，模型就不会知道什么是对的，什么是错的，这是一种无监督学习。


## Reference

